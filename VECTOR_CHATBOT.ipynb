{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ü§ñ Vector Chatbot with Memory\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Sayandip05/vector_chatbot/blob/main/VECTOR_CHATBOT.ipynb)\n",
        "\n",
        "Complete implementation using ChromaDB and sentence transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# STEP 1: INSTALL DEPENDENCIES\n",
        "# ========================================\n",
        "print(\"üì¶ Installing dependencies...\")\n",
        "!pip -q install chromadb sentence-transformers transformers accelerate einops torch\n",
        "print(\"‚úÖ Installation complete!\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# STEP 2: IMPORT LIBRARIES\n",
        "# ========================================\n",
        "print(\"üìö Importing libraries...\")\n",
        "\n",
        "import os\n",
        "import time\n",
        "import uuid\n",
        "import json\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import List, Dict, Optional, Tuple\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "print(\"‚úÖ Libraries imported!\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# STEP 3: CONFIGURATION\n",
        "# ========================================\n",
        "print(\"‚öôÔ∏è Setting up configuration...\")\n",
        "\n",
        "# Model Settings\n",
        "CHAT_MODEL = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
        "EMBED_MODEL = \"all-MiniLM-L6-v2\"\n",
        "\n",
        "# Database Settings\n",
        "PERSIST_DIR = \"/content/memory_db\"\n",
        "COLLECTION_NAME = \"chat_memory\"\n",
        "\n",
        "# Chat Settings\n",
        "MAX_NEW_TOKENS = 256\n",
        "TEMPERATURE = 0.7\n",
        "TOP_K_RETRIEVAL = 6\n",
        "CONTEXT_CHAR_LIMIT = 1600\n",
        "\n",
        "# User Settings\n",
        "USER_ID = \"colab_user\"\n",
        "CONVERSATION_ID = \"conv_001\"\n",
        "\n",
        "# System Prompt\n",
        "SYSTEM_PROMPT = (\n",
        "    \"You are a helpful and friendly assistant. \"\n",
        "    \"When you remember something from past conversations, mention it naturally. \"\n",
        "    \"Keep responses concise and helpful.\"\n",
        ")\n",
        "\n",
        "# Create persist directory\n",
        "os.makedirs(PERSIST_DIR, exist_ok=True)\n",
        "print(f\"‚úÖ Configuration ready! Database: {PERSIST_DIR}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# STEP 4: EMBEDDING HANDLER CLASS\n",
        "# ========================================\n",
        "print(\"üî¢ Creating Embedding Handler...\")\n",
        "\n",
        "class EmbeddingHandler:\n",
        "    \"\"\"Converts text to vector embeddings\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = EMBED_MODEL):\n",
        "        print(f\"  Loading embedding model: {model_name}...\")\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "        print(f\"  ‚úÖ Embedding model loaded!\")\n",
        "\n",
        "    def embed_texts(self, texts: List[str]) -> List[List[float]]:\n",
        "        \"\"\"Convert texts to embeddings\"\"\"\n",
        "        vectors = self.model.encode(\n",
        "            texts,\n",
        "            convert_to_numpy=True,\n",
        "            normalize_embeddings=True\n",
        "        )\n",
        "        return [vec.tolist() for vec in vectors]\n",
        "\n",
        "    def embed_single(self, text: str) -> List[float]:\n",
        "        \"\"\"Convert single text to embedding\"\"\"\n",
        "        return self.embed_texts([text])[0]\n",
        "\n",
        "print(\"‚úÖ Embedding Handler ready!\\n\")\n",
        "\n",
        "# ========================================\n",
        "# STEP 5: MEMORY ITEM CLASS\n",
        "# ========================================\n",
        "\n",
        "@dataclass\n",
        "class MemoryItem:\n",
        "    \"\"\"Single memory entry\"\"\"\n",
        "    id: str\n",
        "    role: str\n",
        "    content: str\n",
        "    user_id: str\n",
        "    conversation_id: str\n",
        "    created_at: float"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# STEP 6: MEMORY MANAGER CLASS\n",
        "# ========================================\n",
        "print(\"üíæ Creating Memory Manager...\")\n",
        "\n",
        "class MemoryManager:\n",
        "    \"\"\"Manages conversation memory\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        print(\"  Initializing ChromaDB...\")\n",
        "\n",
        "        # Create embedder\n",
        "        self.embedder = EmbeddingHandler()\n",
        "\n",
        "        # Setup ChromaDB\n",
        "        self.client = chromadb.PersistentClient(\n",
        "            path=PERSIST_DIR,\n",
        "            settings=Settings(anonymized_telemetry=False)\n",
        "        )\n",
        "\n",
        "        self.collection = self.client.get_or_create_collection(\n",
        "            name=COLLECTION_NAME,\n",
        "            metadata={\"hnsw:space\": \"cosine\"}\n",
        "        )\n",
        "\n",
        "        print(f\"  ‚úÖ Memory ready! Stored items: {self.collection.count()}\")\n",
        "\n",
        "    def add(self, items: List[MemoryItem]):\n",
        "        \"\"\"Add memories\"\"\"\n",
        "        if not items:\n",
        "            return\n",
        "\n",
        "        texts = [item.content for item in items]\n",
        "        embeddings = self.embedder.embed_texts(texts)\n",
        "\n",
        "        self.collection.add(\n",
        "            ids=[item.id for item in items],\n",
        "            documents=texts,\n",
        "            metadatas=[asdict(item) for item in items],\n",
        "            embeddings=embeddings\n",
        "        )\n",
        "\n",
        "    def retrieve(self, query: str, top_k: int = 6) -> List[Tuple[str, Dict, float]]:\n",
        "        \"\"\"Retrieve relevant memories\"\"\"\n",
        "        query_embedding = self.embedder.embed_single(query)\n",
        "\n",
        "        results = self.collection.query(\n",
        "            query_embeddings=[query_embedding],\n",
        "            n_results=top_k,\n",
        "            where={\"user_id\": USER_ID},\n",
        "            include=[\"documents\", \"metadatas\", \"distances\"]\n",
        "        )\n",
        "\n",
        "        docs = results.get(\"documents\", [[]])[0]\n",
        "        metas = results.get(\"metadatas\", [[]])[0]\n",
        "        dists = results.get(\"distances\", [[]])[0]\n",
        "\n",
        "        return list(zip(docs, metas, dists))\n",
        "\n",
        "    def build_context(self, retrieved: List[Tuple[str, Dict, float]]) -> str:\n",
        "        \"\"\"Build context from memories\"\"\"\n",
        "        if not retrieved:\n",
        "            return \"(no relevant memory)\"\n",
        "\n",
        "        chunks = []\n",
        "        used_chars = 0\n",
        "\n",
        "        for i, (doc, meta, dist) in enumerate(retrieved, start=1):\n",
        "            chunk = f\"[memory {i}] {meta.get('role')}: {doc.strip()}\\n\"\n",
        "\n",
        "            if used_chars + len(chunk) > CONTEXT_CHAR_LIMIT:\n",
        "                break\n",
        "\n",
        "            chunks.append(chunk)\n",
        "            used_chars += len(chunk)\n",
        "\n",
        "        return \"\\n\".join(chunks) if chunks else \"(no relevant memory)\"\n",
        "\n",
        "    def create_memory_item(self, role: str, content: str) -> MemoryItem:\n",
        "        \"\"\"Create new memory\"\"\"\n",
        "        return MemoryItem(\n",
        "            id=str(uuid.uuid4()),\n",
        "            role=role,\n",
        "            content=content,\n",
        "            user_id=USER_ID,\n",
        "            conversation_id=CONVERSATION_ID,\n",
        "            created_at=time.time()\n",
        "        )\n",
        "\n",
        "    def count(self) -> int:\n",
        "        \"\"\"Get memory count\"\"\"\n",
        "        return self.collection.count()\n",
        "\n",
        "    def search(self, query: str, top_k: int = 5) -> List[Dict]:\n",
        "        \"\"\"Search memories\"\"\"\n",
        "        results = self.retrieve(query, top_k=top_k)\n",
        "\n",
        "        formatted = []\n",
        "        for doc, meta, dist in results:\n",
        "            formatted.append({\n",
        "                \"role\": meta.get(\"role\"),\n",
        "                \"content\": doc,\n",
        "                \"distance\": round(dist, 3),\n",
        "                \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\",\n",
        "                                          time.localtime(meta.get(\"created_at\", 0)))\n",
        "            })\n",
        "\n",
        "        return formatted\n",
        "\n",
        "print(\"‚úÖ Memory Manager ready!\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# STEP 7: CHATBOT CLASS\n",
        "# ========================================\n",
        "print(\"ü§ñ Creating Chatbot...\")\n",
        "\n",
        "class Chatbot:\n",
        "    \"\"\"Main chatbot with memory\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"  INITIALIZING CHATBOT\")\n",
        "        print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "        # Initialize memory\n",
        "        self.memory = MemoryManager()\n",
        "\n",
        "        # Load chat model\n",
        "        print(f\"  Loading chat model: {CHAT_MODEL}...\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(CHAT_MODEL)\n",
        "\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            CHAT_MODEL,\n",
        "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "\n",
        "        self.pipeline = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=self.model,\n",
        "            tokenizer=self.tokenizer\n",
        "        )\n",
        "\n",
        "        print(\"  ‚úÖ Chat model loaded!\")\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"  ‚úÖ CHATBOT READY!\")\n",
        "        print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "    def generate_response(self, messages: List[Dict[str, str]]) -> str:\n",
        "        \"\"\"Generate model response\"\"\"\n",
        "        prompt = self.tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True\n",
        "        )\n",
        "\n",
        "        output = self.pipeline(\n",
        "            prompt,\n",
        "            max_new_tokens=MAX_NEW_TOKENS,\n",
        "            do_sample=True,\n",
        "            temperature=TEMPERATURE,\n",
        "            top_p=0.9,\n",
        "            top_k=50\n",
        "        )\n",
        "\n",
        "        generated_text = output[0][\"generated_text\"][len(prompt):]\n",
        "        return generated_text.strip()\n",
        "\n",
        "    def chat(self, user_message: str) -> str:\n",
        "        \"\"\"Main chat function\"\"\"\n",
        "        # Retrieve context\n",
        "        retrieved = self.memory.retrieve(user_message, top_k=TOP_K_RETRIEVAL)\n",
        "        context = self.memory.build_context(retrieved)\n",
        "\n",
        "        # Build messages\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"PAST CONTEXT:\\n{context}\\n\\nQUESTION: {user_message}\"\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        # Generate response\n",
        "        response = self.generate_response(messages)\n",
        "\n",
        "        # Store in memory\n",
        "        user_item = self.memory.create_memory_item(\"user\", user_message)\n",
        "        assistant_item = self.memory.create_memory_item(\"assistant\", response)\n",
        "        self.memory.add([user_item, assistant_item])\n",
        "\n",
        "        return response\n",
        "\n",
        "    def search_memory(self, query: str, top_k: int = 5) -> List[Dict]:\n",
        "        \"\"\"Search conversation history\"\"\"\n",
        "        return self.memory.search(query, top_k=top_k)\n",
        "\n",
        "    def memory_count(self) -> int:\n",
        "        \"\"\"Get total memories\"\"\"\n",
        "        return self.memory.count()\n",
        "\n",
        "    def seed_memory(self, conversations: List[tuple]):\n",
        "        \"\"\"Seed initial memories\"\"\"\n",
        "        items = []\n",
        "        for role, content in conversations:\n",
        "            items.append(self.memory.create_memory_item(role, content))\n",
        "\n",
        "        self.memory.add(items)\n",
        "        print(f\"‚úì Seeded {len(items)} memories\\n\")\n",
        "\n",
        "print(\"‚úÖ Chatbot class ready!\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# STEP 8: INITIALIZE CHATBOT\n",
        "# ========================================\n",
        "print(\"üöÄ Starting chatbot initialization...\\n\")\n",
        "\n",
        "bot = Chatbot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# STEP 9: SEED INITIAL MEMORIES\n",
        "# ========================================\n",
        "print(\"üå± Seeding initial memories...\")\n",
        "\n",
        "bot.seed_memory([\n",
        "    (\"user\", \"My name is Alex\"),\n",
        "    (\"assistant\", \"Nice to meet you, Alex!\"),\n",
        "    (\"user\", \"I prefer Python examples over JavaScript\"),\n",
        "    (\"assistant\", \"Got it! I'll use Python for examples.\"),\n",
        "    (\"user\", \"I'm learning machine learning\"),\n",
        "    (\"assistant\", \"That's great! ML is fascinating!\")\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# STEP 10: INTERACTIVE CHAT FUNCTION\n",
        "# ========================================\n",
        "\n",
        "def chat_with_bot():\n",
        "    \"\"\"Interactive chat function\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üéØ CHATBOT STARTED!\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"Commands:\")\n",
        "    print(\"  ‚Ä¢ Just type to chat\")\n",
        "    print(\"  ‚Ä¢ 'search: <query>' - Search memory\")\n",
        "    print(\"  ‚Ä¢ 'count' - Show total memories\")\n",
        "    print(\"  ‚Ä¢ 'exit' - Stop chatting\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            user_input = input(\"You: \").strip()\n",
        "\n",
        "            if not user_input:\n",
        "                continue\n",
        "\n",
        "            # Exit\n",
        "            if user_input.lower() in [\"exit\", \"quit\", \"stop\"]:\n",
        "                print(\"\\nüëã Goodbye!\\n\")\n",
        "                break\n",
        "\n",
        "            # Memory count\n",
        "            if user_input.lower() == \"count\":\n",
        "                count = bot.memory_count()\n",
        "                print(f\"üìä Total memories: {count}\\n\")\n",
        "                continue\n",
        "\n",
        "            # Memory search\n",
        "            if user_input.lower().startswith(\"search:\"):\n",
        "                query = user_input[7:].strip()\n",
        "                if query:\n",
        "                    print(f\"\\nüîç Searching for: '{query}'\")\n",
        "                    results = bot.search_memory(query, top_k=3)\n",
        "\n",
        "                    if results:\n",
        "                        for i, result in enumerate(results, 1):\n",
        "                            print(f\"\\n[{i}] {result['role']} (similarity: {result['distance']})\")\n",
        "                            print(f\"    Time: {result['timestamp']}\")\n",
        "                            print(f\"    Content: {result['content']}\")\n",
        "                    else:\n",
        "                        print(\"No results found.\")\n",
        "                    print()\n",
        "                continue\n",
        "\n",
        "            # Regular chat\n",
        "            print(\"Assistant: \", end=\"\", flush=True)\n",
        "            response = bot.chat(user_input)\n",
        "            print(response + \"\\n\")\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\n\\nüëã Goodbye!\\n\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error: {e}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# STEP 11: DEMO CONVERSATIONS\n",
        "# ========================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìù DEMO CONVERSATIONS\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "demo_questions = [\n",
        "    \"What's my name?\",\n",
        "    \"What programming language do I prefer?\",\n",
        "    \"What am I learning?\",\n",
        "]\n",
        "\n",
        "for question in demo_questions:\n",
        "    print(f\"You: {question}\")\n",
        "    response = bot.chat(question)\n",
        "    print(f\"Assistant: {response}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# STEP 12: START INTERACTIVE CHAT\n",
        "# ========================================\n",
        "\n",
        "# Uncomment to start interactive chat\n",
        "# chat_with_bot()\n",
        "\n",
        "# Or use these commands directly:\n",
        "# response = bot.chat(\"Your message here\")\n",
        "# print(response)\n",
        "#\n",
        "# results = bot.search_memory(\"query\")\n",
        "# for r in results:\n",
        "#     print(r)\n",
        "#\n",
        "# print(f\"Total memories: {bot.memory_count()}\")"
      ]
    }
  ]
}
