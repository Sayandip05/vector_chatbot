# ğŸ¤– Vector Chatbot with Memory

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Sayandip05/vector_chatbot/blob/main/VECTOR_CHATBOT.ipynb)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

A smart chatbot that **remembers your conversations** using vector embeddings and ChromaDB! ğŸ§ âœ¨

## ğŸ¯ What Does This Do?

Imagine talking to a chatbot that actually **remembers** what you told it before - your name, preferences, previous questions - just like talking to a real person! This project makes that happen.

### Simple Example:
```
You: My name is Sarah
Bot: Nice to meet you, Sarah!

[Later...]
You: What's my name?
Bot: Your name is Sarah!
```

## âœ¨ Key Features

- **ğŸ§  Memory System** - Remembers all your conversations
- **ğŸ” Smart Search** - Finds relevant past conversations instantly
- **ğŸ’¬ Natural Chat** - Uses AI to give helpful responses
- **ğŸ“Š Context Aware** - Uses past messages to give better answers
- **ğŸš€ Easy to Use** - Just run in Google Colab, no setup needed!

## ğŸ› ï¸ How It Works

1. **You send a message** â†’ "I love Python programming"
2. **Bot converts to vectors** â†’ Turns your message into numbers
3. **Stores in database** â†’ Saves it in ChromaDB memory
4. **Retrieves context** â†’ When you ask something, it finds related memories
5. **Generates response** â†’ Uses AI model + memories to reply smartly

### Example Flow:
```
User: "I'm learning machine learning"
Bot: "That's great! ML is fascinating!"

[Next day...]
User: "Can you help me with coding?"
Bot: "Sure! Since you're learning ML, I'll use Python examples!" âœ¨
```

## ğŸš€ Quick Start

### Option 1: Google Colab (Easiest!)
1. Click the **"Open in Colab"** badge above
2. Run all cells (Runtime â†’ Run all)
3. Start chatting!

### Option 2: Local Setup
```bash
# Clone the repository
git clone https://github.com/Sayandip05/vector_chatbot.git
cd vector_chatbot

# Install dependencies
pip install chromadb sentence-transformers transformers accelerate einops torch

# Open the notebook
jupyter notebook VECTOR_CHATBOT.ipynb
```

## ğŸ’¡ Usage Examples

### Basic Chat
```python
# Initialize the bot
bot = Chatbot()

# Have a conversation
response = bot.chat("Hi! My name is Alex")
print(response)  # "Nice to meet you, Alex!"

response = bot.chat("What's my name?")
print(response)  # "Your name is Alex!"
```

### Search Past Conversations
```python
# Search your conversation history
results = bot.search_memory("Python", top_k=3)
for result in results:
    print(f"{result['role']}: {result['content']}")
```

### Interactive Chat
```python
# Start an interactive chat session
chat_with_bot()
```

## ğŸ“š What You'll Learn

- ğŸ”¢ **Vector Embeddings** - How to turn text into numbers
- ğŸ’¾ **Vector Databases** - How ChromaDB stores and retrieves data
- ğŸ¤– **AI Chat Models** - Using Hugging Face transformers
- ğŸ§  **Memory Systems** - Building chatbots that remember
- ğŸ” **Semantic Search** - Finding similar conversations

## ğŸ—ï¸ Project Structure

```
vector_chatbot/
â”‚
â”œâ”€â”€ VECTOR_CHATBOT.ipynb    # Main notebook with complete code
â”œâ”€â”€ README.md               # This file
â”œâ”€â”€ LICENSE                 # MIT License
â””â”€â”€ .gitignore             # Git ignore file
```

## ğŸ§© Components

### 1. **Embedding Handler**
Converts your text into vector embeddings using `sentence-transformers`.

### 2. **Memory Manager**
Stores and retrieves conversations using ChromaDB vector database.

### 3. **Chatbot**
The main AI that uses `Qwen2.5-0.5B-Instruct` model to generate responses.

## ğŸ® Commands

When chatting with the bot:

- **Just type** - Regular conversation
- **`search: <query>`** - Search past conversations
- **`count`** - Show total memories stored
- **`exit`** - Stop chatting

## ğŸ”§ Configuration

You can customize these settings in the notebook:

```python
CHAT_MODEL = "Qwen/Qwen2.5-0.5B-Instruct"  # AI model
EMBED_MODEL = "all-MiniLM-L6-v2"            # Embedding model
MAX_NEW_TOKENS = 256                        # Response length
TEMPERATURE = 0.7                           # Creativity (0-1)
TOP_K_RETRIEVAL = 6                         # Memories to retrieve
```

## ğŸ“Š Example Output

```
ğŸ¯ CHATBOT STARTED!
============================================================
Commands:
  â€¢ Just type to chat
  â€¢ 'search: <query>' - Search memory
  â€¢ 'count' - Show total memories
  â€¢ 'exit' - Stop chatting
============================================================

You: Hi! My favorite color is blue
Assistant: That's nice! Blue is a calming color.

You: What's my favorite color?
Assistant: Your favorite color is blue!

You: search: color
ğŸ” Searching for: 'color'

[1] user (similarity: 0.123)
    Time: 2025-01-15 10:30:45
    Content: My favorite color is blue

[2] assistant (similarity: 0.234)
    Time: 2025-01-15 10:30:46
    Content: That's nice! Blue is a calming color.

You: count
ğŸ“Š Total memories: 4
```

## ğŸ¤” How Does Memory Work?

Think of it like this:

1. **ğŸ—£ï¸ You say something** â†’ "I like pizza"
2. **ğŸ”¢ Converts to numbers** â†’ [0.23, 0.45, 0.67, ...]
3. **ğŸ’¾ Stores in database** â†’ Saved with timestamp and metadata
4. **ğŸ” Later, you ask** â†’ "What food do I like?"
5. **ğŸ¯ Searches similar** â†’ Finds "I like pizza" (similar vectors)
6. **ğŸ’¬ Responds smartly** â†’ "You like pizza!"

## ğŸŒŸ Use Cases

- **Personal Assistant** - Remembers your preferences
- **Customer Support** - Recalls previous issues
- **Learning Companion** - Tracks what you're studying
- **Journaling Bot** - Remembers your daily entries
- **Task Manager** - Keeps context of your projects

## ğŸ›¡ï¸ Privacy Note

All conversations are stored **locally** in your environment:
- In Colab: Stored in `/content/memory_db/` (temporary)
- Locally: Stored in your project folder
- No data is sent to external servers (except AI model API calls)

## ğŸ› Troubleshooting

**Problem:** "CUDA out of memory"
```python
# Use CPU instead
CHAT_MODEL = "Qwen/Qwen2.5-0.5B-Instruct"  # Smaller model
```

**Problem:** "Module not found"
```bash
pip install chromadb sentence-transformers transformers
```

**Problem:** Slow responses
- Use a smaller model
- Reduce `MAX_NEW_TOKENS`
- Reduce `TOP_K_RETRIEVAL`

## ğŸ“– Learn More

- [ChromaDB Documentation](https://docs.trychroma.com/)
- [Sentence Transformers](https://www.sbert.net/)
- [Hugging Face Transformers](https://huggingface.co/docs/transformers)
- [Vector Embeddings Explained](https://www.pinecone.io/learn/vector-embeddings/)

## ğŸ¤ Contributing

Contributions are welcome! Feel free to:

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Submit a pull request

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **ChromaDB** - For the vector database
- **Hugging Face** - For the AI models
- **Sentence Transformers** - For embeddings
- **Qwen Team** - For the chat model

## ğŸ“§ Contact

Have questions? Found a bug? Want to share your project?

- Create an [Issue](https://github.com/Sayandip05/vector_chatbot/issues)
- Star â­ the repo if you find it useful!

---

<div align="center">

**Made with â¤ï¸ by [Sayandip05](https://github.com/Sayandip05)**

â­ Star this repo if you found it helpful!

</div>
